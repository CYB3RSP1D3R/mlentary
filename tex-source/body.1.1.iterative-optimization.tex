\objectives{%
  \item implement gradient descent for any given loss function and (usually)
       thereby automatically and efficiently find nearly-optimal linear
       hypotheses from data
  \item explain why the gradient-update formulas for common linear models are
        sensible, not just formally but also intuitively
}

%\samquote{
%  Hey Jude, don't make it bad \\
%  Take a sad song and make it better \\
%  Remember to let her under your skin \\
%  Then you'll begin to make it \\
%  Better, better, better, better, better, better, ...
%}{paul mccartney, john lennon}

        %-- gradients
        %-- writing out the code : a key exercise ; batches
        %-- setting initialization and learning rate; local minima
        %-- visualizing noise and curvature

\sampassage{(stochastic) gradient descent}
  We seek a hypothesis that is best (among a class $\hH$) according to some
  notion of how well each hypothesis models given data:
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    def badness(h,y,x):
        # return e.g. whether h misclassifies y,x OR h's surprise at seeing y,x OR etc
    def badness_on_dataset(h, examples):
        return np.mean([badness(h,y,x) for y,x in examples])
  \end{lstlisting}
        %#return np.mean([for y,x in examples])

  %For example, out notion of goodness might map $h$ to its
  %training accuracy $1-\Ein$.  Or, when $h$ has a probabilistic interpretation,
  %our notion of goodness might map $h$ to the probability it predicts for the
  %training outputs $y_i$.\bovinenote{%
  %  In either case, we view our notion-of-good, computed on the training data,
  %  as an estimate of the notion-of-good we most care about: testing
  %  performance.  So $1-\Ein$ estimates $1-\Eout$ and $p(y_i|x_i;h)$ for $y_i,
  %  x_i$ a training example estimates $p(y|x;h)$ for $y,x$ fresh data.
  %}
  %
  Earlier we found a nearly best candidate by brute-force search over all
  hypotheses.  But this doesn't scale to most interesting cases wherein $\hH$
  is intractably large.
  %
  So: \emph{what's a faster algorithm to find a nearly best candidate?}

  A common idea is to start arbitrarily with some $h_0\in \hH$ and
  repeatedly improve to get $h_1, h_2, \cdots$.  We eventually stop, say at $h_{10000}$.
  The key question is:\bovinenote{%
    Also important are the questions of where to start and when to stop.
    But have patience!  We'll discuss these later.
  }
  \emph{how do we compute an improved hypothesis $h_{t+1}$ from our current
  hypothesis $h_t$}?

  We \emph{could} just keep randomly nudging $h_t$ until we hit on an
  improvement; then we define $h_{t+1}$ as that improvement.  Though this
  sometimes works surprisingly well,\bovinenote{%
    If you're curious, search `metropolis hastings' and
    `probabilistic programming'.
  } we can often save time by exploiting more available information.
  Specifically, we can inspect $h_t$'s inadequacies to inform our proposal
  $h_{t+1}$.
  %
  Intuitively, if $h_t$ misclassifies a particular $(x_i, y_i) \in \sS$, then
  we'd like $h_{t+1}$ to be like $h_t$ but nudged toward
  accurately classifying $(x_i, y_i)$.\bovinenote{%
    In doing better on the $i$th datapoint, we might mess up how we do
    on the other datapoints!  We'll consider this in due time.
  }

  How do we compute ``{a nudge toward accurately classifying $(x, y)$}''?  That
  is, how do measure how slightly changing a parameter affects some result?
  Answer: derivatives!  To make $h$ less bad on an example $(y, x)$, we'll
  nudge $h$ in tiny bit along $-g = -d \texttt{badness}(h,y,x) /
  dh$. Say, $h$ becomes $h-0.01g$.\bovinenote{%
    E.g.\ if each $h$ is a vector and we've chosen
    $\texttt{badness}(h,y,x) = -y h\cdot x$ as our notion of badness, then $-d
    \texttt{badness}(h,y,x) / dh = +yx$, so we'll nudge $h$ in the
    direction of $+yx$.
    \exercise{Is this update familiar?}
  }
  Once we write
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    def gradient_badness(h,y,x):
        # returns the derivative of badness(h,y,x) with respect to h
    def gradient_badness_on_dataset(h, examples):
        return np.mean([gradient_badness(h,y,x) for y,x in examples])
  \end{lstlisting}
  we can repeatedly nudge via \textbf{gradient descent (GD)}, the engine of ML:\bovinenote{%
    \noparexercise{Can GD directly minimize misclassification rate?}
  }
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    h = initialize()
    for t in range(10000):
      h = h - 0.01 * gradient_badness_on_dataset(h, examples)
  \end{lstlisting}
  Since the derivative of total badness depends on all the training data,
  looping $10000$ times is expensive.  So in practice we estimate the needed
  derivative based on some \emph{subset} (jargon: \textbf{batch}) of the
  training data --- a different subset each pass through the loop --- in what's
  called \textbf{stochastic gradient descent (SGD)}:
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    h = initialize()
    for t in range(10000):
      batch = select_subset_of(examples)
      h = h - 0.01 * gradient_badness(h, batch)
  \end{lstlisting}
  \begin{marginfigure}
      \attnsam{cartoon of GD}\\
      \vspace{4cm}\\
      \attnsam{cartoon of GD}
  \end{marginfigure}

  (S)GD requires informative derivatives.  Misclassification rate has
  uninformative derivatives: any tiny change in $h$ won't change the predicted
  labels.  But when we use probabilistic models, small changes in $h$ can lead
  to small changes in the predicted \emph{distribution} over labels.
  %
  To speak poetically: the softness of probabilistic models paves a smooth ramp
  over the intractably black-and-white cliffs of `right' or `wrong'.
  %
  We now apply SGD to maximizing probabilities.

\sampassage{maximum likelihood estimation}
  When we can compute each hypothesis $h$'s asserted probability
  that the training $y$s match the training $x$s, it seems
  reasonable to seek an $h$ for which this probability is maximal.  This
  method is \textbf{maximum likelihood estimation (MLE)}.
  %
  It's convenient for the overall goodness to be a sum (or average) over each
  training example.  But independent chances multiply rather than add:
  rolling snake-eyes has chance $1\!/\!6 \cdot 1\!/\!6$, not $1\!/\!6 + 1\!/\!6$.  So
  we prefer to think about maximizing log-probabilities instead of maximizing
  probabilities --- it's the same in the end.\bovinenote{%
    Throughout this course we make a crucial assumption that our training
    examples are independent from each other.
  }
  By historical
  convention we like to minimize badness rather than maximize goodness, so
  we'll use SGD to \emph{minimize negative-log-probabilities}.
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    def badness(h,y,x):
        return -np.log( probability_model(y,x,h) )
  \end{lstlisting}

  Let's see this in action for the linear logistic model we developed for soft
  binary classification.  A hypothesis $\vec w$ predicts that a (featurized)
  input $\vec x$ has label $y=+1$ or $y=-1$ with chance $\sigma(+ \vec w \cdot \vec x)$
  or $\sigma(- \vec w \cdot \vec x)$:
  $$
    p_{\sfy|\sfx,\sfw}(y|\vec x,\vec w) = \sigma(y \vec w \cdot \vec x)
    \quad\quad
    \text{where}
    \quad\quad
    \sigma(\frd) = 1/(1-\exp(-\frd))
  $$
  So MLE with our logistic model means finding $\vec w$ that \emph{minimizes}
  $$
    -\log\wrap{\text{prob of all $y_i$s given all $\vec x_i$s and $\vec w$}}
    =
    \sum_i -\log(\sigma(y_i \vec w\cdot \vec x_i))
  $$
  The key computation is the derivative of those badness terms:\bovinenote{%
    Remember that $\sigma\pr(z) = \sigma(z)\sigma(-z)$.
    %
    To reduce clutter we'll temporarily write $y \vec w\cdot \vec x$ as $ywx$.
  }
  $$
    \frac{\partial (-\log(\sigma(y w x)))}{\partial w}
    =
    \frac{-\sigma(y w x)\sigma(-y w x) y x}{\sigma(y w x)}
    =
    - \sigma(-y w x) y x
  $$

  \exercise{If you're like me, you might've zoned out by now.  But this stuff
  is important, especially for deep learning!  So please graph the
  above expressions to convince yourself that our formula for derivative
  makes sense visually.}

  \vspace{\baselineskip}

  To summarize, we've found the loss gradient for the logistic model:
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    sigma = lambda z : 1./(1+np.exp(-z))
    def badness(w,y,x):             return -np.log( sigma(y*w.dot(x)) )
    def gradient_badness(w,y,x):    return -sigma(-y*w.dot(x)) * y*x
  \end{lstlisting}
  As before, we define overall badness on a dataset as an average badness over
  examples; and for simplicity, let's intialize gradient descent at $h_0=0$:
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    def gradient_badness_on_dataset(h, examples):
      return np.mean([gradient_badness(h,y,x) for y,x in examples])
    def initialize():
        return np.zeros(NUMBER_OF_DIMENSIONS, dtype=np.float32)
  \end{lstlisting}
  Then we can finally write gradient descent:
  \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily]
    h = initialize()
    for t in range(10000):
      h = h - 0.01 * gradient_badness_on_data(h, examples)
  \end{lstlisting}

  %\attnsam{mention convexity and convergence?}

  \begin{marginfigure}
     \attnsam{show trajectory in weight space over time -- see how certainty
  degree of freedom is no longer redundant? (``markov'')}\\
      \vspace{4cm}\\
    \attnsam{show training and testing loss and acc over time}
  \end{marginfigure}



  \newpage
\sampassage{initialization, learning rate, local minima}\marginnote{\veryoptional}

\sampassage{pictures of training: noise and curvature}\marginnote{\veryoptional}
  \par\attnsam{}
  \par\attnsam{}
  \par\attnsam{test vs train curves: overfitting}
  \par\attnsam{random featurization: double descent}

\sampassage{practical implementation: vectorization}


%    \samsection{5. ideas in optimization}
%      \samquote{
%        premature optimization is the root of all evil
%      }{donald knuth}
%
%        \attn{learning rate as metric; robustness to 2 noise structures}
%        \attn{nesterov momentum}
%        \attn{decaying step size; termination conditions}
%        \attn{batch normalization}
%
%
%
%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%%~~~~~~~~~~~~~  2.20. local minima  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%      \sampassage{local minima}
%        % convexity, initialization
%
%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%%~~~~~~~~~~~~~  2.21. implicit regularization  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%      \sampassage{implicit regularization}
%
%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%%~~~~~~~~~~~~~  2.22. learning rate schedule  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%      \sampassage{learning rate schedule}
%
%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%%~~~~~~~~~~~~~  2.23. learning rates as dot products  ~~~~~~~~~~~~~~~~~~~~~~~~~
%
%      \sampassage{learning rates as dot products} % connects to whitening / pre-conditioning; ties into next section on kernels
%
%
