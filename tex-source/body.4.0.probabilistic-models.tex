%\subsection*{Lecture 4a: Probabilistic Models}

% =============================================================================
% ==  _  ======================================================================
% =============================================================================

\sampassage{Probabilistic Models}
%
%-------  _  ------------------------------------------------------------------
\blurb{Data comes from somewhere; expressivity through richer output-side nonlinearities that we interpret probabilistically}
%
In this unit we explore the idea that \emph{data comes from somewhere}.  The data that
we typically care about arose through processes we can to good approximation say a
great deal about; then by a kind of reverse engineering of this process we can
design good inference algorithms for this data.  Probability theory often
gives a good language for describing these processes: some random occurrence
happened and based on that some other random occurrence happened and so forth
until our data gets measured, so that overall we get this very complicated
probability distribution whose richness we want to model.
%
This fact --- that data comes from somewhere --- and this probabilistic
attitude toward this fact will inspire us toward even richer and more
sophisticated and more domain tailored architectures for our machine learning
models.

Indeed, we started in Unit 1 with linear models.  In Units 2 and 3 we enriched
those models by pre-processing our input data (the input of our linear map)
through sophisticated, domain-tailored non-linear featurizers.  Now, in Unit 4,
we will post-process the decision function value (the output of our linear map)
through sophisticated, domain-tailored probability-related nonlinearities.

We've already seen two very small examples of this post-processing.  First,
softmax.  Softmax takes a vector (the one that our linear map outputs) and
squashes it into a probability distribution over a fixed finite set of
possibilities.  Then minimizing logistic loss is just maximum likelihood for
this family of conditional probability distributions.  Second, least squares
regression.  When we do regression, we get some numeric output for each input.
We usually don't expect the output to be exactly correct; instead, we interpret
the model as predicting that the truth will be close to its output.  To be
definite, let's regard the model as stating that the truth follows some fixed
variance Gaussian centered on the output.  Then minimizing square loss of a
regression model is the same as maximizing likelihood, if we interpret the
linear map's output as parameterizing the mean of a fixed-variance Gaussian.
%Now we more thoroughly explore the possibilities of parameterizing possible
%families of distributions that might have something to do with our actual task
%as we can tell using domain knowledge in the above two examples we parameterize
%the space of all possible distributions were fixed finite set of possibilities
%that soft max or prime or tries a very simple family of boring old couch and
%they're all the same shape but the shepherd relative to each other in least
%squares regression.

In either case, the linear map's output picks out a particular distribution
over the set $\yY$ of distributions, out of a family of such distributions.
the post-processing we mention is simply the implementation of this picking out
--- jargon: \emph{parameterization} --- step.  So our overall architecture will
look like this: we take the input $x$ in $\xX$; then we feed it through some
nonlinearity; then we apply our learned linear layer; then the output of that
layer we regard as the parameters of some probability distribution over $\yY$.

That probability distribution exists only mathematically in the sense that in
RAM it's just represented as its parameters.  Still, we can usually do things
like a sample from the distribution to get a diversity of concrete $y$s, or
figure out the chance of any given $y$ relative to the distribution.  So our
architecture overall represents a family (parameterized by weights $\theta$) of
conditional distributions $p_{\sfy|\sfx;\theta}$ --- or, what's the same, a
family of stochastic functions from $\xX$ to $\yY$.

An aspect of this enrichment is that the learning problem look quite
interesting even in the very simple case where there's only one possible input
$x$ in $\xX$.  That is, when $\xX$ has size one.  Then our examples are just a
bunch of $y$s and we seek to fit a distribution to that bunch.  This is
so-called \emph{unsupervised learning}.  Supervised learning and unsupervised
learning are both instances of learning from examples.  \emph{Supervised
learning} is the jargon that we use when we are especially interested in the
challenge of learning input output relationships.  \emph{Unsupervised learning}
is the jargon we use when what's more interesting to us is modeling the rich
structure of the outputs.  The distinction is as blurry and as practically
useful at the distinction between a hotdog and a sandwich.

For most of this unit, we will focus on unsupervised learning, since the
probability stuff is the new stuff and the input-output stuff we've already
seen.  We will occasionally sprinkle in examples with nontrivial input-output
relationships, to show how that would work.  Only near the end of this unit
will we do this in depth.

%-------  _  ------------------------------------------------------------------
\blurb{example: beyond gaussian for regression}
%
Our architectures so far model uncertainty crudely.
For example, when we train a neural network regression model $f$ with
weights $\theta$ and square loss, we are maximizing the following
likelihood function (with independence assumption on training data):
% no NNs and linear reg
% folks have not read previous notes in detail
$$
  p_{\sfy|\sfx}(y|x;\theta) = \text{gaussian distribution with mean $f(x;\theta)$ and variance $1$}
$$
% or two examples
In many real-world scenarios, however, the ``true'' distribution of $\sfy$
conditioned on $\sfx=x$ is much richer than a gaussian distribution: it might
have multiple humps, heavier tails, or correlations with other predictands.
Think, for instance, of predicting the a person's distribution of
daily-app-usages based on their age and job and country (but not on the day):
for a musician maybe some days have a lot of tuning and sheet music usage and
other days (vacations) have none, but with no days having intermediate levels.
This is non-gaussian.

When the space $Y$ of possible outputs is high-dimensional (e.g., a space of
all images), there are even more ways for the true likelihood functions' shapes
shape to differ from a gaussian.

\attnsam{TODO can some volunteer make an image of some $30\times 30$ color
 images?  Want six images: one of a cow, one of a dog, one halfway in between
 as a vector of dimension $30\cdot 30\cdot 3$, one gaussianly sampled with
 i.i.d. gaussian colors centered at gray and with stddev 0.25 of total
 brightness range (clip at black and white), one gaussianly sampled with
 independent gaussian colors centered at the cow image and with stddev 0.25 of
 total brightness range (clip at black and white), and one gaussianly sampled
 according to the PCA of 500 cow and dog images?  Ask sam for details}

The distribution of all natural grayscale images is not gaussian when
we represent images in ``raw'' pixel-brightness coordinates.  I mean
that we are thinking about multi-variate gaussians with as many
dimensions as there are pixels.  Natural images are all and only those
photographs taken by professional photographers in the past four
decades, made into grayscale, and cropped to a standard shape.
%
\exercise{What would a gaussian distribution in image space look
like with spherical covariance?  Arbitrary covariance?  (Don't worry
much about overflow beyond the maximum and minimum brightnesses).}
%
\exercise{What are qualitative aspects of the distribution of natural images
that this does not model?}
%
\par Here is one possible answer:
In a scenario where the distribution over natural images is gaussian, then the
pixelwise average of any two images would be more probable than at least one of
the images.  But if (in aforementioned coordinates) we average a cat image and
a dog image, we get an unclear image (rather than a clear image of an imaginary
hybrid mammal).
\attnsam{TODO: Draw and insert two-panel CARTOON of the above property of
gaussians vs of multimodal distributions (in jargon this illustrates one aspect
of the failure of the image probability density to be convex)}

%-------  _  ------------------------------------------------------------------
\blurb{forward picture of distribution (generative model)}

So in this unit, we want to treat the construction of richer, more
domain-appropriate probability models as a design challenge, a degree of
freedom we have as engineers.  We'll go beyond the defaults such as square loss
implicit in previous units.  In doing so, we will engage with new facets of
approximation.  And the language we develop will clarify for us aspects of
generalization that pertain to this as well as previous units, which will
suggest to us a new and harder optimization goal.  So we will then develop new
ideas in optimization toward that goal.

That's abstract.  The models we'll think about concretely in this unit
are combinations of deterministic transforms and sampling statements.
For example, the following is a model of length-$N$ lists $[y_0,
\cdots, y_{N-1}]$ of numbers.
\begin{table*}[h]
\begin{tabular}{l}
    %   \text{sample $c_a$ from a normal distribution with mean $0$ and variance $10$}
    %\\ \text{sample $c_b$ from a normal distribution with mean $0$ and variance $10$}
       \text{given is a number $c_a$}
    \\ \text{given is a number $c_b$}
    \\ \text{for $0\leq n<N$:}
    \\ \text{~~~~~~~~sample $f_n$ by a coin flip that lands $a$ with chance $0.8$ else $b$}
    \\ \text{~~~~~~~~sample $y_n$ by normal distribution with mean $c_{f_n}$ and variance $1$}
\end{tabular}
\end{table*}
FOODFORTHOUGHT: For $c_a=-5$ and $c_b=+5$, what might a 1D
scatterplot of $y_n$s look like?

To model $(x,y)$ pairs we might do something like:
\begin{table*}[h]
\begin{tabular}{l}
    \text{given is a weight vector $\theta_a$}
    \text{given is a weight vector $\theta_b$}
    \\ \text{for $0\leq n<N$:}
    \\ \text{~~~~~~~~sample $f_n$ by a coin flip that lands $a$ with chance $0.8$ else $b$}
    \\ \text{~~~~~~~~sample $y_n$ by normal distribution with mean $\theta_{f_n} \cdot x_n$ and variance $1$}
\end{tabular}
\end{table*}

By contrast, the models we have looked at in previous units don't
really do sampling except of $y$ at the end (and maybe of $\theta$
at the beginning, for the prior).

TODO: compare (discuss conditional, compare to supervised; initial samplers priors and regularizer?)
%\begin{table*}[h]
%\begin{tabular}{l}
%    \text{sample $\theta_a$ from a normal distribution with mean $0$ and variance $10$}
%    \\ \text{sample $\theta_b$ from a normal distribution with mean $0$ and variance $10$}
%    \\ \text{for $0\leq n<N$:}
%    \\ \text{~~~~~~~~sample $c_n$ by a coin flip that lands $a$ with chance $0.9$ else $b$}
%    \\ \text{~~~~~~~~sample $y_n$ by normal distribution with mean $\theta_{c_n}$ and variance $1$}
%\end{tabular}
%\end{table*}

%-------  _  ------------------------------------------------------------------
\blurb{latents and marginalization} %and generalization}

  %As you can see,
  %focus on unsupervised learning

%-------  _  ------------------------------------------------------------------
\blurb{point estimate vs distribution; utilities; square loss muddies}
% bayes decision theory?

% =============================================================================
% ==  _  ======================================================================
% =============================================================================

\sampassage{Example: 3State Traffic.  Summation.}
%
%-------  _  ------------------------------------------------------------------
\blurb{3state traffic model definition}
%
Now we describe a toy example we'll use throughout this Unit.  There are $3$
possible weathers each day in each city: Sunny, Hailing, and Foggy (SHF).  We
choose a city whose coldness ranges thru $[0,1]$ (Warm thru Cold).  And there
are $2$ possible traffics: NoJam or Jam.   In a completely Warm city or a
completely Cool city, the probabilities of weather-traffic pairs are
as given in the table in the margin.  Cities with intermediate coldnesses linearly interpolate between these two
tables.
%
  %\begin{table}[h!]
      \begin{marginfigure}[-2cm]
      \centering
      \begin{tabular}{cccc}
        \textsc{warm}:&  &Noj & Jam \\%\hrule
              & S& .5 &0  \\
              & H& 0  &.25\\
              & F& 0  &.25
      \end{tabular}\\
      %\hspace{2cm}%
      %\begin{tabular}{cccc}
      %  \textsc{medium}:&  &Noj & Jam \\%\hrule
      %        & S& .3 &0  \\
      %        & H& .2 &.15\\
      %        & F& .2 &.15
      %\end{tabular}
      %\hspace{2cm}%
      \begin{tabular}{cccc}
        %\textsc{cold}:& & Noj & Jam \\%\hrule
        \textsc{cold}:& &     &     \\%\hrule
             &S&.1  &0  \\
             &H&.4  &.05\\
             &F&.4  &.05
      \end{tabular}
  \end{marginfigure}
  %\end{table}

The idea is that warm cities more often have sunny weather than cold cities.
And that sunny weather tends to lead to less traffic jams ---
but in cold cities, where folks are used to non-sunny weather, the effect
of weather on jams is less.

% TODO: three-jam state (for U curve)

We observe the traffic variable on a single day (one bit of information), and
we want to estimate the city coldness (one real number in $[0,1]$).  This toy
is very simple --- misleadingly simple in some ways.
%
We can solve this model exactly by thinking about it.\bovinenote{%
\noparexercise{%
%How would you compute the maximum likelihood estimate for
%coldness given Nojam?  That is, which numbers above are involved
%in the arithmetic, and how are they involved in the arithmetic?
Which case (Jam or NoJam) leads to a higher Coldness estimate?}
}
Still, we'll soon use it
to illustrate heavy-duty data crunching techniques.  That way we can compare
with the exact right answers.


%-------  _  ------------------------------------------------------------------
\blurb{Sampling and Graphical Notation}
%-------  _  ------------------------------------------------------------------
\blurb{Inference with Known Parameter}
%-------  _  ------------------------------------------------------------------
\blurb{Parameter Estimation}
%-------  _  ------------------------------------------------------------------
\blurb{Challenge of Summation}

% =============================================================================
% ==  _  ======================================================================
% =============================================================================

\sampassage{Example: Parameterizing Spread.  Gaussian Mixture Models.  D-separation.}
%
%-------  _  ------------------------------------------------------------------
\blurb{1-cluster case: forward model, generation, and inference}
Recall that in least squares regression, we're trying to find weights $\theta$
that determine a function $f_\theta:\xX\to \yY=\Rr$ that maximizes the
likelihood of observing the training data under a model where $y_n$ is sampled
from fixed-variance gaussians around $f_\theta(x_n)$.  What if we instead
let the machine figure out appropriate variances so that it can tell us, based
on data, when it is less confident?

%\attnsam{TODO: }
\attnsam{TODO: plot of two samples from fixed-variance model and two samples
from varying-variance model}

What that would look like is a pair of functions $f_\theta,g_\theta:\xX\to\Rr$
that determine, for each input $x$, a gaussian distribution for $y$-values at
$x$.  One way to do this is to regard $f$ as the gaussian's mean and $g$ as the
gaussian's variance.  Actually, since we don't want to allow negative
variances, it is more convenient to regard $\exp g$ as the gaussian's variance.
Therefore, the likelihood for a given training sample $(x,y)$ is
$$
  \exp\wrap{-\frac{1}{2}\wrap{%
    (y-f_\theta(x))^2/\exp g_\theta(x)
    +
    \log(2\pi) + g_\theta(x)
    }}
$$

Intuitively, doing regression this way allows the model to ``give up'' on
regions of $\xX$ where inference seems too noisy and thus focus/spend the
model's expressivity on tractable regions instead of wasting it.
\attnsam{TODO: offer intuitive situation; make plot}

We could do gradient descent on (the negative log of) the above likelihood,
just as we do in ordinary least squares, to find a good $\theta$.  For example,
perhaps $\xX$ has $30$ features and $\theta$ has $(30+1)+(30+1)=62$ dimensions,
where the first $30+1$ parameterize the linear function $f_\theta$ (with bias)
and the second $30+1$ parameterize the linear function $g_\theta$ (with bias).

This situation is fun and useful.  It's already worth studying and quite useful
in the case where $\xX$ only contains one $x$-value.  Then we are fitting a
gaussian of varying mean and variance to our training data's $y$-values.
\exercise{In this gaussian-fitting situation, make up some training data.  Can
you visualize the loss landscape, i.e., the function from weightspace (here
two-dimensional) to loss?  Define loss as negative log likelihood, as usual.
Why is there just one minimum?  What do the level curves look like?  How do
they differ from ellipses?}


\exercise{For river level regression prediction, what are reasons it might be
useful to get the machine to provide calibrated assesments of the possible
$y$-values' spread instead of just their mean?}

%-------  _  ------------------------------------------------------------------
\blurb{enriching 1 cluster to 2 clusters.  GMM definition.  use cases.}
We stick to the case $|\xX|=1$ for now to focus on the interesting new ideas in
modeling a distribution over $\yY$.  Before, we modeled spread just as the
variance of a gaussian.  But there are further, interesting ways a distribution
can be spread.  For example, it might have a fat rightward tail (like annual
the distribution of household incomes in the united states) or it might have
multiple humps (like the distribution of longitudes humans live in).  To model
having multiple humps, we could parameterize a \emph{mixture of gaussians}, a
probability density that looks like a weighted average of multiple different
gaussian densities.  For example, to model two humps we'd have as our parameters
\emph{two} means, \emph{two} variances, and a parameter saying how much mass
is in one hump vs the other:
\begin{align*}
  p(y) =  
    &\text{weight}_0 \exp\wrap{-\frac{1}{2}\wrap{%
    (y-\text{mean}_0)^2/\exp \text{logvar}_0
    +
    \log(2\pi) + \text{logvar}_0
    }}
    +
    \\&\text{weight}_1 \exp\wrap{-\frac{1}{2}\wrap{%
    (y-\text{mean}_1)^2/\exp \text{logvar}_1
    +
    \log(2\pi) + \text{logvar}_1
    }}
\end{align*}
Here, the two weights add to one, so in effect they constitute one parameter.

This is the ($2$-hump, $1$-dimensional case of) of the probability model called
the \textbf{Gaussian Mixture Model (GMM)}.  By fitting a GMM to data, we can
learn about the hidden structure of the data such as its underlying clusters,
generate new examples in the same spirit (by sampling from the probability
distribution), test whether future examples are anomalies (by seeing whether or
not they are likely according to the learned distribution), classify examples
by which hump they more likely belong to, etc.  DATA MODELING, DATA SCIENCE,
VERY RICH THING TO PROBE

\attnsam{TODO: add CONCRETE EXAMPLES of kinds of data for which this might
be interesting.  Include a diversity of domains, e.g. medicine, aero-astro,
etc}

%-------  _  ------------------------------------------------------------------
\blurb{question of fitting.  graphical model.  explaining away.}

There's a slight sickness in the multi-hump likelihood formula above when we
are inferring parameters: we can get arbitrarily high likelihoods by letting
one of the clusters very narrowly center on a datapoint!
%
There are three

From a more down-to-earth perspective, we'd like to have high testing time
likelihood rather than training set likelihood.  CROSS-VALIDATION

UTILITY VS BITS

PRIORS, REGULARIZATION: add $\sum_k \lambda/\exp \text{logvar}_k$ to the loss
function.

\attnsam{TODO: draw and interpret graphical model of GMM, including prior}

Now a short remark on a very important phenomenon called EXPLAINING AWAY: if
there are two possible explanations for an observation, then new observations
that cause us to increase our probability that one explanation happened will
cause us to decrease our probability that the other explanation happened, even
if those explanations are not mutually exclusive and a priori those
explanations are independent!  SHIFTING TENSION/BURDEN OF EXPLANATION
%
We can see this effect very spatially as different gaussian components
``negotiate'' their territories.  \attnsam{TODO: plots!}

%-------  _  ------------------------------------------------------------------
\blurb{sampling from a 3-cluster GMM to make color / stick figure data}

%-------  _  ------------------------------------------------------------------
\blurb{example of conditional 1-cluster, 2-cluster GMM to connect to unit 3.  color data}

%%-------  _  ------------------------------------------------------------------
%\blurb{GMM definition}

%%-------  _  ------------------------------------------------------------------
%\blurb{graphical notation}
%
%%-------  _  ------------------------------------------------------------------
%\blurb{generation, the question of fitting, and use cases}
%
%%-------  _  ------------------------------------------------------------------
%\blurb{priors on priors}
%
%%-------  _  ------------------------------------------------------------------
%\blurb{hierarchy and transfer}

% =============================================================================
% ==  _  ======================================================================
% =============================================================================

\sampassage{Example: hidden markov models.  Hierarchy and Transfer Learning.}

%-------  _  ------------------------------------------------------------------
\blurb{definition of HMM}
Here is an example of a probabilistic model for
generating length-$T+1$ sequences of labels from a finite set $L$ of
label values.
\begin{table*}[h]
\begin{tabular}{l}
       \text{given is a distribution $\iota(\cdot)$ on $L$}
    \\ \text{given is a conditional distribution $\tau(\cdot|\cdot)$ from $L$ to $L$}
    \\ \text{sample $l_0$ from $\iota(\cdot)$}
    \\ \text{for $0\leq t<T$:}
    \\ \text{~~~~~~~~sample $l_{t+1}$ from $\tau(\cdot|l_t)$}
\end{tabular}
\end{table*}
MARKOV PROPERTY

%-------  _  ------------------------------------------------------------------
\blurb{}
Adding a layer of sophistication, we might imagine
that our observation at timestep $t$ is related to but not the same as
the evolving state.
\begin{table*}[h]
\begin{tabular}{l}
       \text{given is a distribution $\iota(\cdot)$ on $Z$}
    \\ \text{given is a conditional distribution $\tau(\cdot|\cdot)$ from $Z$ to $Z$}
    \\ \text{given is a conditional distribution $\epsilon(\cdot|\cdot)$ from $Z$ to $L$}
    \\ \text{sample $z_0$ from $\iota(\cdot)$}
    \\ \text{for $0\leq t<T$:}
    \\ \text{~~~~~~~~sample $z_{t+1}$ from $\tau(\cdot|z_t)$}
    \\ \text{for $0\leq t\leq T$:}
    \\ \text{~~~~~~~~sample $l_{t}$ from $\epsilon(\cdot|z_t)$}
\end{tabular}
\end{table*}

FAILURE OF MARKOV PROPERTY

%-------  _  ------------------------------------------------------------------
\blurb{Expressivity: Generating Cool Sequences}
%-------  _  ------------------------------------------------------------------
\blurb{Inference given Weights: 0s and Softlogic and Backflow}
%-------  _  ------------------------------------------------------------------
\blurb{Inference of Weights, Qualitatively.  Hierarchy and Transfer.}

% =============================================================================
% ==  _  ======================================================================
% =============================================================================

\sampassage{Latents, Marginal Likelihood for Latents, Weights as Latents.}

%-------  _  ------------------------------------------------------------------
\blurb{Weights as Latents.}
%-------  _  ------------------------------------------------------------------
\blurb{Point vs Distribution estimates, and generalization.}
%-------  _  ------------------------------------------------------------------
\blurb{Bowtie vs Pipe}
%-------  _  ------------------------------------------------------------------
\blurb{BIC as an approximation.  Double descent.}
%-------  _  ------------------------------------------------------------------
\blurb{Beyond BIC}


