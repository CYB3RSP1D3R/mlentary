    \samsection{1. learned featurizations}
      \sampassage{imagining the space of feature tuples}
        We'll focus on an architecture of the form
        $$
          \hat p(y\!=\!+1\,|\,x) \,=\,
          (\sigma_{1\times 1} \circ
          A_{1\times (h+1)} \circ
          f_{(h+1)\times h} \circ
          B_{h\times d})(x)
        $$
        where $A,B$ are linear maps with the specified
        $(\text{input}\times\text{output})$ dimensions, where $\sigma$ is the
        familiar sigmoid operation, and where $f$ applies the leaky relu
        function elementwise and concatenates a $1$:
        $$
          f((v_i : 0\leq i<h)) = (1,)\,+\!\!\!\!+\,(\text{lrelu}(v_i) : 0\leq i<h)
          \quad \quad
          \text{lrelu}(z) = \max(z/10, z)
        $$
        We call $h$ the \textbf{hidden dimension} of the model.
        %
        Intuitively, $f \circ B$ re-featurizes the input to a form more
        linearly separable (by weight vector $A$).

      \sampassage{the featurization layer's learning signal}
      \sampassage{expressivity and local minima}% approximation % logic, etc
      \sampassage{``representer theorem''}


